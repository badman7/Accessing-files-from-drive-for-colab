{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERFNet",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpQnG+GDWIRx2lYcoznMy8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cskarthik7/Accessing-files-from-drive-for-colab/blob/master/ERFNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWowHZO_wsoW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bea270d0-323a-4627-ede6-b6a6b84697e2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-8o8ToC1KaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets,transforms\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.utils.data import sampler\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHNLZpSwzBEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca6b325b-3f31-4310-a731-d86752178735"
      },
      "source": [
        "cd '/content/drive/My Drive'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxC0N3iT16QY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c56831ce-1c89-420d-f4a1-6d59d261f395"
      },
      "source": [
        "cd '/content/drive/My Drive/erfnet/eval'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/erfnet/eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv5minospSJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install visdom\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2kHQTRtpm9f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "df7a68d2-115f-47d5-bd60-72e36fe7006c"
      },
      "source": [
        "!python eval_cityscapes_color.py --datadir \"../dataset/\" --subset val "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model: ../trained_models/erfnet.py\n",
            "Loading weights: ../trained_models/erfnet_pretrained.pth\n",
            "Model and weights LOADED successfully\n",
            "torch.Size([1, 16, 256, 512])\n",
            "torch.Size([1, 16, 256, 512])\n",
            "torch.Size([1, 64, 128, 256])\n",
            "torch.Size([1, 128, 64, 128])\n",
            "../dataset/1.png\n",
            "0 ../dataset/1.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLyfM-NZqFW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat eval_cityscapes_color.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYwRtLlZcn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat dataset.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-89SzUTlSIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat erfnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPU5PHEfSkno",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e76415c6-2be2-4eb5-c729-76bb76a02a0c"
      },
      "source": [
        "%%writefile eval_cityscapes_color.py\n",
        "# Code to produce colored segmentation output in Pytorch for all cityscapes subsets  \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "import visdom\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize((512,1024),Image.BILINEAR),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize((512,1024),Image.NEAREST),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "])\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    modelpath = args.loadDir + args.loadModel\n",
        "    weightspath = args.loadDir + args.loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "  \n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not args.cpu):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(args.datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset),\n",
        "        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
        "\n",
        "    # For visualizer:\n",
        "    # must launch in other window \"python3.6 -m visdom.server -port 8097\"\n",
        "    # and access localhost:8097 to see it\n",
        "    if (args.visualize):\n",
        "        vis = visdom.Visdom()\n",
        "    cnt=1\n",
        "    for step, (images,filename) in enumerate(loader):\n",
        "        if (not args.cpu):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "\n",
        "        inputs = Variable(images)\n",
        "        #targets = Variable(labels)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        #label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        label_color = Colorize()(label.unsqueeze(0))\n",
        "\n",
        "        filenameSave = \"../dataset/\"+str(cnt)+\".png\"\n",
        "        cnt=cnt+1\n",
        "        print(filenameSave)\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)      \n",
        "        label_save = ToPILImage()(label_color)           \n",
        "        label_save.save(filenameSave) \n",
        "\n",
        "        if (args.visualize):\n",
        "            vis.image(label_color.numpy())\n",
        "        print (step, filenameSave)\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--state')\n",
        "\n",
        "    parser.add_argument('--loadDir',default=\"../trained_models/\")\n",
        "    parser.add_argument('--loadWeights', default=\"erfnet_pretrained.pth\")\n",
        "    parser.add_argument('--loadModel', default=\"erfnet.py\")\n",
        "    parser.add_argument('--subset', default=\"val\")  #can be val, test, train, demoSequence\n",
        "\n",
        "    parser.add_argument('--datadir', default=os.getenv(\"HOME\") + \"/datasets/cityscapes/\")\n",
        "    parser.add_argument('--num-workers', type=int, default=4)\n",
        "    parser.add_argument('--batch-size', type=int, default=1)\n",
        "    parser.add_argument('--cpu', action='store_true')\n",
        "\n",
        "    parser.add_argument('--visualize', action='store_true')\n",
        "    main(parser.parse_args())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting eval_cityscapes_color.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmA65_KPlW0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6114c30e-a46c-4102-fc95-d1d08329d253"
      },
      "source": [
        "%%writefile erfnet.py\n",
        "# ERFNET full network definition for Pytorch\n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DownsamplerBlock (nn.Module):\n",
        "    def __init__(self, ninput, noutput):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(ninput, noutput-ninput, (3, 3), stride=2, padding=1, bias=True)\n",
        "        self.pool = nn.MaxPool2d(2, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = torch.cat([self.conv(input), self.pool(input)], 1)\n",
        "        print(output.shape)\n",
        "        output = self.bn(output)\n",
        "        return F.relu(output)\n",
        "    \n",
        "\n",
        "class non_bottleneck_1d (nn.Module):\n",
        "    def __init__(self, chann, dropprob, dilated):        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1,0), bias=True)\n",
        "\n",
        "        self.conv1x3_1 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1), bias=True)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n",
        "\n",
        "        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated,1))\n",
        "\n",
        "        self.conv1x3_2 = nn.Conv2d(chann, chann, (1,3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n",
        "\n",
        "        self.dropout = nn.Dropout2d(dropprob)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        output = self.conv3x1_1(input)\n",
        "        output = F.relu(output)\n",
        "        output = self.conv1x3_1(output)\n",
        "        output = self.bn1(output)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        output = self.conv3x1_2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.conv1x3_2(output)\n",
        "        output = self.bn2(output)\n",
        "\n",
        "        if (self.dropout.p != 0):\n",
        "            output = self.dropout(output)\n",
        "        \n",
        "        return F.relu(output+input)    #+input = identity (residual connection)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.initial_block = DownsamplerBlock(3,16)\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(DownsamplerBlock(16,64))\n",
        "\n",
        "        for x in range(0, 5):    #5 times\n",
        "           self.layers.append(non_bottleneck_1d(64, 0.1, 1))  \n",
        "\n",
        "        self.layers.append(DownsamplerBlock(64,128))\n",
        "\n",
        "        for x in range(0, 2):    #2 times\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n",
        "            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n",
        "\n",
        "        #only for encoder mode:\n",
        "        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, input, predict=False):\n",
        "        output = self.initial_block(input)\n",
        "        print(output.shape)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "\n",
        "        if predict:\n",
        "            output = self.output_conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class UpsamplerBlock (nn.Module):\n",
        "    def __init__(self, ninput, noutput):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n",
        "        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        output = self.bn(output)\n",
        "        return F.relu(output)\n",
        "\n",
        "class Decoder (nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        self.layers.append(UpsamplerBlock(128,64))\n",
        "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
        "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
        "\n",
        "        self.layers.append(UpsamplerBlock(64,16))\n",
        "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
        "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
        "\n",
        "        self.output_conv = nn.ConvTranspose2d( 16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = input\n",
        "\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "\n",
        "        output = self.output_conv(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class ERFNet(nn.Module):\n",
        "    def __init__(self, num_classes, encoder=None):  #use encoder to pass pretrained encoder\n",
        "        super().__init__()\n",
        "\n",
        "        if (encoder == None):\n",
        "            self.encoder = Encoder(num_classes)\n",
        "        else:\n",
        "            self.encoder = encoder\n",
        "        self.decoder = Decoder(num_classes)\n",
        "\n",
        "    def forward(self, input, only_encode=False):\n",
        "        if only_encode:\n",
        "            return self.encoder.forward(input, predict=True)\n",
        "        else:\n",
        "            output = self.encoder(input)    #predict=False by default\n",
        "            return self.decoder.forward(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting erfnet.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfCmxmRKZhqu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e6c25e0-57a7-4189-f5c5-c4b58778ed8b"
      },
      "source": [
        "%%writefile dataset.py\n",
        "# Code with dataset loader for VOC12 and Cityscapes (adapted from bodokaiser/piwise code)\n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "EXTENSIONS = ['.jpg', '.png']\n",
        "\n",
        "def load_image(file):\n",
        "    return Image.open(file)\n",
        "\n",
        "def is_image(filename):\n",
        "    return any(filename.endswith(ext) for ext in EXTENSIONS)\n",
        "\n",
        "def is_label(filename):\n",
        "    return filename.endswith(\"_labelTrainIds.png\")\n",
        "\n",
        "def image_path(root, basename, extension):\n",
        "    return os.path.join(root, f'{basename}{extension}')\n",
        "\n",
        "def image_path_city(root, name):\n",
        "    return os.path.join(root, f'{name}')\n",
        "\n",
        "def image_basename(filename):\n",
        "    return os.path.basename(os.path.splitext(filename)[0])\n",
        "\n",
        "class VOC12(Dataset):\n",
        "\n",
        "    def __init__(self, root, input_transform=None, target_transform=None):\n",
        "        self.images_root = os.path.join(root, 'images')\n",
        "        self.labels_root = os.path.join(root, 'labels')\n",
        "\n",
        "        self.filenames = [image_basename(f)\n",
        "            for f in os.listdir(self.labels_root) if is_image(f)]\n",
        "        self.filenames.sort()\n",
        "\n",
        "        self.input_transform = input_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "\n",
        "        with open(image_path(self.images_root, filename, '.jpg'), 'rb') as f:\n",
        "            image = load_image(f).convert('RGB')\n",
        "        with open(image_path(self.labels_root, filename, '.png'), 'rb') as f:\n",
        "            label = load_image(f).convert('P')\n",
        "\n",
        "        if self.input_transform is not None:\n",
        "            image = self.input_transform(image)\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "\n",
        "class cityscapes(Dataset):\n",
        "\n",
        "    def __init__(self, root, input_transform=None, target_transform=None, subset='val'):\n",
        "        self.images_root = os.path.join(root)\n",
        "        self.labels_root = os.path.join(root, 'gtFine/' + subset)\n",
        "\n",
        "        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\n",
        "        self.filenames.sort()\n",
        "\n",
        "        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\n",
        "        self.filenamesGt.sort()\n",
        "\n",
        "        self.input_transform = input_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "        #filenameGt = self.filenamesGt[index]\n",
        "\n",
        "        #print(filename)\n",
        "\n",
        "        with open(image_path_city(self.images_root, filename), 'rb') as f:\n",
        "            image = load_image(f).convert('RGB')\n",
        "        #with open(image_path_city(self.labels_root, filenameGt), 'rb') as f:\n",
        "        #    label = load_image(f).convert('P')\n",
        "\n",
        "        if self.input_transform is not None:\n",
        "            image = self.input_transform(image)\n",
        "        #if self.target_transform is not None:\n",
        "        #    label = self.target_transform(label)\n",
        "\n",
        "        return image, filename\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting dataset.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqhMADTHo0Hd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "457bb523-9b8b-4d15-eae8-08bafadd654a"
      },
      "source": [
        "%%writefile eval_cityscapes_server.py\n",
        "# Code to produce segmentation output in Pytorch for all cityscapes subset \n",
        "# Sept 2017\n",
        "# Eduardo Romera\n",
        "#######################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "from PIL import Image\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize\n",
        "from torchvision.transforms import ToTensor, ToPILImage\n",
        "\n",
        "from dataset import cityscapes\n",
        "from erfnet import ERFNet\n",
        "from transform import Relabel, ToLabel, Colorize\n",
        "\n",
        "\n",
        "NUM_CHANNELS = 3\n",
        "NUM_CLASSES = 20\n",
        "\n",
        "image_transform = ToPILImage()\n",
        "input_transform_cityscapes = Compose([\n",
        "    Resize((512,512)),\n",
        "    ToTensor(),\n",
        "    #Normalize([.485, .456, .406], [.229, .224, .225]),\n",
        "])\n",
        "target_transform_cityscapes = Compose([\n",
        "    Resize(512),\n",
        "    ToLabel(),\n",
        "    Relabel(255, 19),   #ignore label to 19\n",
        "])\n",
        "\n",
        "cityscapes_trainIds2labelIds = Compose([\n",
        "    Relabel(19, 255),  \n",
        "    Relabel(18, 33),\n",
        "    Relabel(17, 32),\n",
        "    Relabel(16, 31),\n",
        "    Relabel(15, 28),\n",
        "    Relabel(14, 27),\n",
        "    Relabel(13, 26),\n",
        "    Relabel(12, 25),\n",
        "    Relabel(11, 24),\n",
        "    Relabel(10, 23),\n",
        "    Relabel(9, 22),\n",
        "    Relabel(8, 21),\n",
        "    Relabel(7, 20),\n",
        "    Relabel(6, 19),\n",
        "    Relabel(5, 17),\n",
        "    Relabel(4, 13),\n",
        "    Relabel(3, 12),\n",
        "    Relabel(2, 11),\n",
        "    Relabel(1, 8),\n",
        "    Relabel(0, 7),\n",
        "    Relabel(255, 0),\n",
        "    ToPILImage(),\n",
        "    Resize(1024, Image.NEAREST),\n",
        "])\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    modelpath = args.loadDir + args.loadModel\n",
        "    weightspath = args.loadDir + args.loadWeights\n",
        "\n",
        "    print (\"Loading model: \" + modelpath)\n",
        "    print (\"Loading weights: \" + weightspath)\n",
        "\n",
        "    #Import ERFNet model from the folder\n",
        "    #Net = importlib.import_module(modelpath.replace(\"/\", \".\"), \"ERFNet\")\n",
        "    model = ERFNet(NUM_CLASSES)\n",
        "\n",
        "    model = torch.nn.DataParallel(model)\n",
        "    if (not args.cpu):\n",
        "        model = model.cuda()\n",
        "\n",
        "    #model.load_state_dict(torch.load(args.state))\n",
        "    #model.load_state_dict(torch.load(weightspath)) #not working if missing key\n",
        "\n",
        "    def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict elements\n",
        "        own_state = model.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name not in own_state:\n",
        "                 continue\n",
        "            own_state[name].copy_(param)\n",
        "        return model\n",
        "\n",
        "    model = load_my_state_dict(model, torch.load(weightspath))\n",
        "    print (\"Model and weights LOADED successfully\")\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if(not os.path.exists(args.datadir)):\n",
        "        print (\"Error: datadir could not be loaded\")\n",
        "\n",
        "\n",
        "    data=cityscapes(args.datadir, input_transform_cityscapes, target_transform_cityscapes, subset=args.subset)\n",
        "    print(len(data))\n",
        "    loader = DataLoader(cityscapes(args.datadir, input_transform_cityscapes,None, subset=args.subset),\n",
        "        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
        "    \n",
        "    \n",
        "    print(len(os.listdir(args.datadir)))\n",
        "    cnt=1\n",
        "    for step, (images, filename) in enumerate(loader):\n",
        "        if (not args.cpu):\n",
        "            images = images.cuda()\n",
        "            #labels = labels.cuda()\n",
        "        print(images.shape)\n",
        "        inputs = Variable(images)\n",
        "        #targets = Variable(labels)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "\n",
        "        label = outputs[0].max(0)[1].byte().cpu().data\n",
        "        label_cityscapes = cityscapes_trainIds2labelIds(label.unsqueeze(0))\n",
        "        #print (numpy.unique(label.numpy()))  #debug\n",
        "\n",
        "        filenameSave = \"../dataset/\"+str(cnt)+\".png\"\n",
        "        cnt=cnt+1\n",
        "        print(filenameSave)\n",
        "        os.makedirs(os.path.dirname(filenameSave), exist_ok=True)\n",
        "        #image_transform(label.byte()).save(filenameSave)\n",
        "        label_cityscapes.save(filenameSave)\n",
        "\n",
        "        print (step, filenameSave)\n",
        "\n",
        "    \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--state')\n",
        "\n",
        "    parser.add_argument('--loadDir',default=\"../trained_models/\")\n",
        "    parser.add_argument('--loadWeights', default=\"erfnet_pretrained.pth\")\n",
        "    parser.add_argument('--loadModel', default=\"erfnet.py\")\n",
        "    parser.add_argument('--subset', default=\"val\")  #can be val, test, train, demoSequence\n",
        "    parser.add_argument('--datadir', default=os.getenv(\"HOME\") + \"../dataset/\")\n",
        "    parser.add_argument('--num-workers', type=int, default=4)\n",
        "    parser.add_argument('--batch-size', type=int, default=1)\n",
        "    parser.add_argument('--cpu', action='store_true')\n",
        "\n",
        "    main(parser.parse_args())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting eval_cityscapes_server.py\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}